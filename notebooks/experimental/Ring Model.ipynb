{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import notebook_importer\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import zlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294967296"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE**(PRECISION * Q_MAXDEGREE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2625  0.2651  0.3413  0.2708\n",
       " 0.3527  0.2913  0.3530  0.4402\n",
       " 0.2449  0.3435  0.4886  0.4565\n",
       " 0.1048  0.1776  0.1880  0.2262\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE = 2\n",
    "\n",
    "PRECISION_INTEGRAL = 8\n",
    "PRECISION_FRACTIONAL = 8\n",
    "PRECISION = PRECISION_INTEGRAL + PRECISION_FRACTIONAL\n",
    "BOUND = BASE**PRECISION\n",
    "\n",
    "# Q field\n",
    "# Q = 6497992661811505123 # < 64 bits\n",
    "Q = 2147483647 # < 32 bits\n",
    "Q_MAXDEGREE = 2\n",
    "# assert Q > BASE**(PRECISION * Q_MAXDEGREE) # supported multiplication degree (without truncation)\n",
    "\n",
    "\n",
    "def encode(rational, field=Q, precision_fractional=PRECISION_FRACTIONAL):\n",
    "    upscaled = rational * BASE**precision_fractional\n",
    "    field_element = upscaled % field\n",
    "    return field_element\n",
    "\n",
    "def decode(field_element, field=Q, precision_fractional=PRECISION_FRACTIONAL):\n",
    "    mask = (field_element <= int(field/2)).double()\n",
    "    upscaled = (field_element * mask) + ((field_element - field) * (1 - mask))\n",
    "    rational = upscaled / BASE**precision_fractional\n",
    "    return rational\n",
    "\n",
    "def share(secret, field=Q):\n",
    "    first  = torch.rand(4,4).double() * field\n",
    "    second = (secret - first) % field\n",
    "    return [first, second]\n",
    "\n",
    "def reconstruct(shares, field=Q):\n",
    "    s = sum(shares) % field\n",
    "    return s\n",
    "\n",
    "def reconstruct_data(shares,field=Q):\n",
    "    return sum(shares) % field\n",
    "\n",
    "a = torch.rand(4,4).double()\n",
    "a_enc = encode(a).floor()\n",
    "a_sh = share(a_enc)\n",
    "\n",
    "b = torch.rand(4,4).double()\n",
    "b_enc = encode(b).floor()\n",
    "\n",
    "c_sh = (b_enc.mm(a_sh[0]),b_enc.mm(a_sh[1]))\n",
    "c_enc_enc = reconstruct(c_sh)\n",
    "c_enc = decode(c_enc_enc)\n",
    "c = decode(c_enc)\n",
    "\n",
    "# compute loss\n",
    "target = torch.eye(4).double()\n",
    "grad = ((c - target))/8\n",
    "\n",
    "# begin backprop\n",
    "grad_enc = encode(grad).floor()\n",
    "\n",
    "b_enc_grad1 = grad_enc.mm(a_sh[0].transpose(0,1))\n",
    "b_enc_grad2 = grad_enc.mm(a_sh[1].transpose(0,1))\n",
    "\n",
    "decode(decode(reconstruct_data([b_enc_grad1,b_enc_grad2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0412 -0.0065  0.0238  0.0395\n",
       " 0.2581  0.1835  0.3691  0.1992\n",
       " 0.3841  0.3004  0.5210  0.3286\n",
       " 0.4906  0.3286  0.5303  0.2489\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = torch.rand(4,4).double()\n",
    "# b = torch.rand(4,4).double()\n",
    "\n",
    "a = Variable(a,requires_grad=True)\n",
    "b = Variable(b,requires_grad=True)\n",
    "c = b.mm(a)\n",
    "loss = nn.MSELoss()(c,Variable(target))\n",
    "loss.backward()\n",
    "\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2223  0.1426  0.2775  0.3583\n",
       " 0.1103  0.0858  0.1361  0.0889\n",
       " 0.2401  0.1669  0.2958  0.3861\n",
       " 0.3302  0.2310  0.3413  0.4243\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_variables\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1641,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# class SharedLinear(nn.Module):\n",
    "    \n",
    "#     def __init__(self,input_dim,output_dim):\n",
    "        \n",
    "#         self.weights = Vtorch.rand(4,4).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1642,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Tensor():\n",
    "    \n",
    "#     def __init__(self,data=torch.rand(4,4).double()):\n",
    "#         if(type(data) == Variable):\n",
    "#             self.data = data\n",
    "#             self.data.data = self.data.data.double()\n",
    "#         else:\n",
    "#             self.data = Variable(data.double())\n",
    "    \n",
    "#     def encode(self):\n",
    "#         self.data = encode(self.data)\n",
    "#         return self\n",
    "    \n",
    "#     def decode(self):\n",
    "#         self.data = decode(self.data)\n",
    "#         return self\n",
    "    \n",
    "#     def share(self):\n",
    "#         first,second = share(self.data)\n",
    "        \n",
    "#     def dot(self,y):\n",
    "#         return Tensor(data=self.data.mm(y.data))\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return str(self.data)\n",
    "    \n",
    "# def SharedTensor():\n",
    "    \n",
    "#     def __init__(self,shares):\n",
    "#         self.shares = shares\n",
    "    \n",
    "#     def dot(self,y):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1643,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Tensor().encode()\n",
    "b = Tensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4504  1.1138  1.6164  0.9516\n",
       " 1.2566  1.0230  1.2962  1.2076\n",
       " 1.4686  1.0174  1.7135  1.4413\n",
       " 1.3000  0.7191  1.1913  1.3451\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 1644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(b).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1645,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1049  0.1783  0.1363  0.1678\n",
       " 0.1887  0.3355  0.2936  0.2936\n",
       " 0.1678  0.2307  0.1678  0.2726\n",
       " 0.0000  0.0000  0.0000  0.0000\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 1646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2427  0.2537  0.2673  0.2825\n",
       " 0.1318  0.2385  0.1412  0.2352\n",
       " 0.2361  0.2097  0.2784  0.3678\n",
       " 0.2019  0.2418  0.1410  0.2400\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 1509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3688  0.3187  0.3516  0.2178\n",
       " 0.4972  0.3987  0.3940  0.2871\n",
       " 0.4098  0.3410  0.2343  0.2304\n",
       " 0.4251  0.4295  0.2830  0.2987\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 1510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1070  0.1845  0.1619  0.1203\n",
       " 0.1533  0.0194  0.1543  0.0824\n",
       " 0.2112  0.1755  0.0234  0.1674\n",
       " 0.1539  0.1474  0.1336 -0.0131\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 1511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((c - target)/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2740  0.3677  0.2537  0.2359\n",
       " 0.1600  0.3115  0.2594  0.1827\n",
       " 0.2715  0.3490  0.2092  0.1978\n",
       " 0.1562  0.3345  0.2068  0.1228\n",
       "[torch.DoubleTensor of size 4x4]"
      ]
     },
     "execution_count": 1481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((c - target)/8).mm(a.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is exploring the use the Chinese remainder theorem (CRT) for operation on multi-precision integers.\n",
    "\n",
    "Note that throughout we don't perform the modulus reduction by default since it'll often be interesting to do so only in a lazy fashion. In fact, below we'll pick our moduli such that we can do a dot product between two vectors of dimension ~1000 and with ~120 bit numbers, while only performing a reduction at the end.\n",
    "\n",
    "TODOs (besides those mentioned in the code):\n",
    "- anything to gain from Barrett reductions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import log, ceil\n",
    "\n",
    "log2 = lambda x: log(x)/log(2)\n",
    "prod = lambda xs: reduce(lambda x, y: x * y, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number theory\n",
    "\n",
    "All we really need here is the ability to finding (ring) inverses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def egcd(a, b):\n",
    "    if a == 0:\n",
    "        return (b, 0, 1)\n",
    "    else:\n",
    "        g, y, x = egcd(b % a, a)\n",
    "        return (g, x - (b // a) * y, y)\n",
    "    \n",
    "def gcd(a, b):\n",
    "    g, _, _ = egcd(a, b)\n",
    "    return g\n",
    "\n",
    "def inverse(a, m):\n",
    "    _, b, _ = egcd(a, m)\n",
    "    return b % m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ring\n",
    "\n",
    "Define the ring in which we're working. For the numbers in the CRT to fit into 64 bit signed words we hence need each modulus to be ~26 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = [89702869, 78489023, 69973811, 70736797, 79637461]\n",
    "for mi in ms: assert 2 * log2(mi) + log2(1024) < 63, mi\n",
    "\n",
    "M = prod(ms)\n",
    "assert log2(M) >= 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also fix a truncation amount in anticipation of fixedpoint arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 2**16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars\n",
    "\n",
    "Introducing these mostly because the ideas are perhaps easier to follow here. Not sure about neither utility nor speed-ups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural number representation\n",
    "\n",
    "Natural (built-in) multi-precision integers mod `N` for performance comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaturalScalar(1329227995784915872903807060280344576)\n",
      "NaturalScalar(1298074214633706907132624082305024)\n"
     ]
    }
   ],
   "source": [
    "class NaturalScalar:\n",
    "    \"\"\" Uses the typical built-in representation of numbers \"\"\"\n",
    "    \n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'NaturalScalar({})'.format(self.unwrap())\n",
    "    \n",
    "    def unwrap(self):\n",
    "        return self.value\n",
    "    \n",
    "    def __add__(x, y):\n",
    "        return NaturalScalar(x.value + y.value)\n",
    "    \n",
    "    def __sub__(x, y):\n",
    "        return NaturalScalar(x.value - y.value)\n",
    "    \n",
    "    def __mul__(x, y):\n",
    "        return NaturalScalar(x.value * y.value)\n",
    "    \n",
    "    def reduce(x):\n",
    "        return NaturalScalar(x.value % M)\n",
    "    \n",
    "    def mod(x):\n",
    "        return NaturalScalar(x.value % K)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample():\n",
    "        return NaturalScalar(random.randrange(M))\n",
    "\n",
    "\n",
    "a = 2**120\n",
    "b = 2**110\n",
    "x = NaturalScalar(a); print(x)\n",
    "y = NaturalScalar(b); print(y)\n",
    "z = x + y; z = z.reduce(); assert z.unwrap() == (a+b) % M, z\n",
    "z = x - y; z = z.reduce(); assert z.unwrap() == (a-b) % M, z\n",
    "z = x * y; z = z.reduce(); assert z.unwrap() == (a*b) % M, z\n",
    "z = x.mod(); assert z.unwrap() == a % K, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRT number representation\n",
    "\n",
    "Alternative representation where numbers are split into several parts that each fit into a 64 bit signed word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_crt():\n",
    "    \n",
    "    # make sure all values in ms are coprime\n",
    "    for i, mi in enumerate(ms):\n",
    "        for j, mj in enumerate(ms[i+1:]):\n",
    "            assert gcd(mi, mj) == 1, '{} and {} are not coprime'.format(mi, mj)\n",
    "    \n",
    "    def decompose(x):\n",
    "        return [ x % mi for mi in ms ]\n",
    "    \n",
    "    # precomputation for recombine\n",
    "    Mis = ( M // mi for mi in ms )\n",
    "    ls = [ Mi * inverse(Mi, mi) % M for Mi, mi in zip(Mis, ms) ]\n",
    "    \n",
    "    def recombine(xs):\n",
    "        return sum( xi * li for xi, li in zip(xs, ls) ) % M\n",
    "    \n",
    "    return decompose, recombine\n",
    "\n",
    "decompose, recombine = gen_crt()\n",
    "\n",
    "assert recombine(decompose(123456789)) == 123456789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mod():\n",
    "    \n",
    "    # precomputation for mod\n",
    "    qs = [ inverse(M//mi, mi) for mi in ms ]\n",
    "    B = M % K\n",
    "    bs = [ (M//mi) % K for mi in ms ]\n",
    "\n",
    "    def mod(xs):\n",
    "        ts = [ (xi * qi) % mi for xi, qi, mi in zip(xs, qs, ms) ]\n",
    "        alpha = round(sum( float(ti) / float(mi) for ti, mi in zip(ts, ms) ))\n",
    "        v = int( sum( ti * bi for ti, bi in zip(ts, bs) ) - B * alpha )\n",
    "        \n",
    "        assert abs(v) < K * sum(ms) # TODO express in bit length\n",
    "        \n",
    "        return decompose(v % K) # TODO inline decompose?\n",
    "    \n",
    "    return mod\n",
    "\n",
    "mod = gen_mod()\n",
    "\n",
    "assert mod(decompose(123456789)) == decompose(123456789 % K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrtScalar([32730343, 65319507, 11926796, 58726713, 67363725])\n",
      "CrtScalar([49526222, 70198023, 21605128, 12422474, 62826948])\n"
     ]
    }
   ],
   "source": [
    "class CrtScalar:\n",
    "    \"\"\" Uses the CRT representation of numbers \"\"\"\n",
    "    \n",
    "    def __init__(self, value, parts=None):\n",
    "        if value is not None:\n",
    "            parts = decompose(value)\n",
    "        self.parts = parts\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'CrtScalar({})'.format(self.parts)\n",
    "    \n",
    "    def unwrap(self):\n",
    "        return recombine(self.parts)\n",
    "    \n",
    "    def __add__(x, y):\n",
    "        # component-wise operation that can be done in parallel\n",
    "        return CrtScalar(None, [ \n",
    "            (xi + yi) for xi, yi in zip(x.parts, y.parts)\n",
    "        ])\n",
    "    \n",
    "    def __sub__(x, y):\n",
    "        # component-wise operation that can be done in parallel\n",
    "        return CrtScalar(None, [ \n",
    "            (xi - yi) for xi, yi in zip(x.parts, y.parts)\n",
    "        ])\n",
    "\n",
    "    def __mul__(x, y):\n",
    "        # component-wise operation that can be done in parallel\n",
    "        return CrtScalar(None, [ \n",
    "            (xi * yi) for xi, yi in zip(x.parts, y.parts)\n",
    "        ])\n",
    "    \n",
    "    def reduce(x):\n",
    "        return CrtScalar(None, [\n",
    "            xi % mi for xi, mi in zip(x.parts, ms)\n",
    "        ])\n",
    "    \n",
    "    def mod(x):\n",
    "        return CrtScalar(None, mod(x.parts))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample():\n",
    "        return CrtScalar(None, [\n",
    "            random.randrange(mi) for mi in ms\n",
    "        ])\n",
    "\n",
    "\n",
    "a = 2**120\n",
    "b = 2**110\n",
    "x = CrtScalar(a); print(x)\n",
    "y = CrtScalar(b); print(y)\n",
    "z = x + y; z = z.reduce(); assert z.unwrap() == (a+b) % M, z\n",
    "z = x - y; z = z.reduce(); assert z.unwrap() == (a-b) % M, z\n",
    "z = x * y; z = z.reduce(); assert z.unwrap() == (a*b) % M, z\n",
    "z = x.mod(); assert z.unwrap() == a % K, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance tests\n",
    "\n",
    "Simple benchmarks between `NaturalScalar` and `CrtScalar`. Note that the time for `CrtScalar` is on a single core, meaning it'll ideally be reduced by a factor 5 on a multi-core device. Scalars are not the main focus though, and not sure how relevant this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaturalScalar  : 0:00:00.086124\n",
      "CrtScalar      : 0:00:00.173483\n"
     ]
    }
   ],
   "source": [
    "for scalar_type in [NaturalScalar, CrtScalar]:\n",
    "    \n",
    "    x = scalar_type(2**120)\n",
    "    \n",
    "    start = datetime.now()\n",
    "    for _ in range(100000):\n",
    "        y = x\n",
    "        z = y * y\n",
    "        #z.reduce() # roughly doubles the execution time for both (bit less for Typical, bit more for Crt)\n",
    "    end = datetime.now()\n",
    "    print('{:15}: {}'.format(scalar_type.__name__, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private scalar\n",
    "\n",
    "Again mostly because of the simpler scalar setting, we here do secret sharing on top of the two different scalar representations. Note that only multiplication with a constant is given here (to avoid bringing in triples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_private_scalar(scalar_type):\n",
    "    \n",
    "    # precomputation for truncation\n",
    "    K_inv = scalar_type(inverse(K, M))\n",
    "    M_wrapped = scalar_type(M)\n",
    "    def raw_truncate(x):\n",
    "        y = x - x.mod()\n",
    "        return y * K_inv\n",
    "    \n",
    "    class AbstractPrivateScalar:\n",
    "\n",
    "        def __init__(self, value, share0=None, share1=None):\n",
    "            if value is not None:\n",
    "                value = scalar_type(value)\n",
    "                share0 = scalar_type.sample()\n",
    "                share1 = value - share0\n",
    "            self.share0 = share0\n",
    "            self.share1 = share1\n",
    "\n",
    "        def __repr__(self):\n",
    "            return 'PrivateScalar({})'.format(self.unwrap())\n",
    "        \n",
    "        def unwrap(self):\n",
    "            return self.reconstruct().unwrap()\n",
    "        \n",
    "        def reconstruct(self):\n",
    "            return (self.share0 + self.share1).reduce()\n",
    "        \n",
    "        def __add__(x, y):\n",
    "            # component-wise operation that can be done in parallel\n",
    "            return AbstractPrivateScalar(None,\n",
    "                share0 = x.share0 + y.share0,\n",
    "                share1 = x.share1 + y.share1\n",
    "            )\n",
    "        \n",
    "        def __sub__(x, y):\n",
    "            # component-wise operation that can be done in parallel\n",
    "            return AbstractPrivateScalar(None,\n",
    "                share0 = x.share0 - y.share0,\n",
    "                share1 = x.share1 - y.share1\n",
    "            )\n",
    "        \n",
    "        def __mul__(x, k):\n",
    "            # component-wise operation that can be done in parallel\n",
    "            return AbstractPrivateScalar(None,\n",
    "                share0 = x.share0 * k,\n",
    "                share1 = x.share1 * k\n",
    "            )\n",
    "        \n",
    "        def reduce(x):\n",
    "            return AbstractPrivateScalar(None,\n",
    "                share0 = x.share0.reduce(),\n",
    "                share1 = x.share1.reduce()\n",
    "            )\n",
    "        \n",
    "        def truncate(x):\n",
    "            return AbstractPrivateScalar(None,\n",
    "                share0 = raw_truncate(x.share0),\n",
    "                share1 = M_wrapped - raw_truncate((M_wrapped - x.share1).reduce())\n",
    "            )\n",
    "            \n",
    "\n",
    "    return AbstractPrivateScalar\n",
    "\n",
    "for scalar_type in [NaturalScalar, CrtScalar]:\n",
    "    \n",
    "    PublicScalar  = scalar_type\n",
    "    PrivateScalar = gen_private_scalar(scalar_type)\n",
    "\n",
    "    a = 2**120\n",
    "    b = 2**110\n",
    "    x = PrivateScalar(a)\n",
    "    y = PrivateScalar(b)\n",
    "    k = PublicScalar(b)\n",
    "    \n",
    "    z = x + y; z = z.reduce(); assert z.unwrap() == (a+b) % M, z\n",
    "    z = x - y; z = z.reduce(); assert z.unwrap() == (a-b) % M, z\n",
    "    z = x * k; z = z.reduce(); assert z.unwrap() == (a*b) % M, z\n",
    "    z = x.truncate(); assert z.unwrap() in [a // K, a // K + 1], z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance tests\n",
    "\n",
    "A few simple tests. Again the cost for `CrtScalar` would be cut in five on a multi-core device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaturalScalar  : 0:00:00.029406\n",
      "CrtScalar      : 0:00:00.041221\n"
     ]
    }
   ],
   "source": [
    "for scalar_type in [NaturalScalar, CrtScalar]:\n",
    "    \n",
    "    PublicScalar  = scalar_type\n",
    "    PrivateScalar = gen_private_scalar(scalar_type)\n",
    "    \n",
    "    x = PrivateScalar(2**120)\n",
    "    k = PublicScalar(2**110)\n",
    "    \n",
    "    start = datetime.now()\n",
    "    for _ in range(10000):\n",
    "        z = x * k\n",
    "        #z.reduce() # roughly doubles the execution time for both (bit less for Typical, bit more for Crt)\n",
    "    end = datetime.now()\n",
    "    print('{:15}: {}'.format(scalar_type.__name__, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "The ideas above carried over to the tensor setting using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural tensors\n",
    "\n",
    "Tensors backed by a NumPy array of numbers of type `object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaturalTensor:\n",
    "    \"\"\" Uses the typical built-in representation of numbers \"\"\"\n",
    "    \n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'NaturalTensor({})'.format(self.unwrap())\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.values.shape\n",
    "    \n",
    "    def unwrap(self):\n",
    "        return self.values\n",
    "    \n",
    "    def __add__(x, y):\n",
    "        return NaturalTensor(x.values + y.values)\n",
    "    \n",
    "    def __sub__(x, y):\n",
    "        return NaturalTensor(x.values - y.values)\n",
    "    \n",
    "    def __mul__(x, y):\n",
    "        return NaturalTensor(x.values * y.values)\n",
    "    \n",
    "    def dot(x, y):\n",
    "        return NaturalTensor(x.values.dot(y.values))\n",
    "    \n",
    "    def reduce(x):\n",
    "        return NaturalTensor(x.values % M)\n",
    "    \n",
    "    def mod(x):\n",
    "        return NaturalTensor(x.values % K)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample(shape):\n",
    "        return NaturalTensor(np.array([ random.randrange(M) for _ in range(prod(shape)) ]).reshape(shape))\n",
    "    \n",
    "a = np.array([ 2**120 for _ in range(1024) ])\n",
    "b = np.array([ 2**110 for _ in range(1024) ])\n",
    "x = NaturalTensor(a)\n",
    "y = NaturalTensor(b)\n",
    "z = x + y; z = z.reduce(); assert (z.unwrap() == (a+b) % M).all(), z\n",
    "z = x - y; z = z.reduce(); assert (z.unwrap() == (a-b) % M).all(), z\n",
    "z = x * y; z = z.reduce(); assert (z.unwrap() == (a*b) % M).all(), z\n",
    "z = x.dot(y); z = z.reduce(); assert z.unwrap() == a.dot(b) % M, z\n",
    "z = y.mod(); assert (z.unwrap() == b % K).all(), z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRT tensors\n",
    "\n",
    "Tensors backed by a NumPy array of numbers of type `int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrtTensor:\n",
    "    \"\"\" Uses the CRT representation of numbers \"\"\"\n",
    "    \n",
    "    def __init__(self, values, parts=None):\n",
    "        if values is not None:\n",
    "            parts = [ part.astype(np.int64) for part in decompose(values) ]\n",
    "        self.parts = parts\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'CrtTensor({})'.format(self.parts)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.parts[0].shape\n",
    "    \n",
    "    def unwrap(self):\n",
    "        return recombine(self.parts)\n",
    "    \n",
    "    def __add__(x, y):\n",
    "        # component-wise operation that can be done in parallel\n",
    "        return CrtTensor(None, [ \n",
    "            (xi + yi) for xi, yi in zip(x.parts, y.parts)\n",
    "        ])\n",
    "    \n",
    "    def __sub__(x, y):\n",
    "        # component-wise operation that can be done in parallel\n",
    "        return CrtTensor(None, [ \n",
    "            (xi - yi) for xi, yi in zip(x.parts, y.parts)\n",
    "        ])\n",
    "\n",
    "    def __mul__(x, y):\n",
    "        # component-wise operation that can be done in parallel\n",
    "        return CrtTensor(None, [ \n",
    "            (xi * yi) for xi, yi in zip(x.parts, y.parts)\n",
    "        ])\n",
    "    \n",
    "    def dot(x, y):\n",
    "        return CrtTensor(None, [ \n",
    "            xi.dot(yi) for xi, yi in zip(x.parts, y.parts)\n",
    "        ])\n",
    "    \n",
    "    def reduce(x):\n",
    "        return CrtTensor(None, [\n",
    "            xi % mi for xi, mi in zip(x.parts, ms)\n",
    "        ])\n",
    "    \n",
    "    # TODO: straight-forward from scalar types, just need to take NumPy into account\n",
    "#     def mod(x):\n",
    "#         return CrtTensor(None, mod(x.parts))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample(shape):\n",
    "        return CrtTensor(None, [\n",
    "            np.random.randint(mi, size=shape) for mi in ms\n",
    "        ])\n",
    "\n",
    "\n",
    "a = np.array([ 2**120 for _ in range(1024) ])\n",
    "b = np.array([ 2**110 for _ in range(1024) ])\n",
    "x = CrtTensor(a)\n",
    "y = CrtTensor(b)\n",
    "z = x + y; z = z.reduce(); assert (z.unwrap() == (a+b) % M).all(), z\n",
    "z = x - y; z = z.reduce(); assert (z.unwrap() == (a-b) % M).all(), z\n",
    "z = x * y; z = z.reduce(); assert (z.unwrap() == (a*b) % M).all(), z\n",
    "z = x.dot(y); z = z.reduce(); assert z.unwrap() == a.dot(b) % M, z\n",
    "# z = y.mod(); assert (z.unwrap() == b % K).all(), z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance tests\n",
    "\n",
    "We can finally get to do benchmarks on dot products. `CrtTensor` is ~10x faster already on single core device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaturalTensor  : 0:00:01.153654\n",
      "CrtTensor      : 0:00:00.116858\n"
     ]
    }
   ],
   "source": [
    "for tensor_type in [NaturalTensor, CrtTensor]:\n",
    "    \n",
    "    x = tensor_type(np.array([ 2**120 for _ in range(1024) ]))\n",
    "    k = tensor_type(np.array([ 2**110 for _ in range(1024) ]))\n",
    "    \n",
    "    start = datetime.now()\n",
    "    for _ in range(10000):\n",
    "        z = x.dot(k)\n",
    "        z.reduce() # only little effect compared to dot\n",
    "    end = datetime.now()\n",
    "    print('{:15}: {}'.format(tensor_type.__name__, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private tensors\n",
    "\n",
    "Again bring in secret sharing just to illustrate that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_private_tensor(tensor_type):\n",
    "    \n",
    "    class AbstractPrivateTensor:\n",
    "\n",
    "        def __init__(self, values, shares0=None, shares1=None):\n",
    "            if values is not None:\n",
    "                values = tensor_type(values)\n",
    "                shares0 = tensor_type.sample(values.shape)\n",
    "                shares1 = values - shares0\n",
    "            self.shares0 = shares0\n",
    "            self.shares1 = shares1\n",
    "\n",
    "        def __repr__(self):\n",
    "            return 'PrivateTensor({})'.format(self.unwrap())\n",
    "        \n",
    "        def unwrap(self):\n",
    "            return self.reconstruct().unwrap()\n",
    "        \n",
    "        def reconstruct(self):\n",
    "            return (self.shares0 + self.shares1).reduce()\n",
    "        \n",
    "        def __add__(x, y):\n",
    "            # component-wise operation that can be done in parallel\n",
    "            return AbstractPrivateTensor(None,\n",
    "                shares0 = x.shares0 + y.shares0,\n",
    "                shares1 = x.shares1 + y.shares1\n",
    "            )\n",
    "        \n",
    "        def __sub__(x, y):\n",
    "            # component-wise operation that can be done in parallel\n",
    "            return AbstractPrivateTensor(None,\n",
    "                shares0 = x.shares0 - y.shares0,\n",
    "                shares1 = x.shares1 - y.shares1\n",
    "            )\n",
    "        \n",
    "        def __mul__(x, k):\n",
    "            # component-wise operation that can be done in parallel\n",
    "            return AbstractPrivateTensor(None,\n",
    "                shares0 = x.shares0 * k,\n",
    "                shares1 = x.shares1 * k\n",
    "            )\n",
    "        \n",
    "        def dot(x, k):\n",
    "            # component-wise operation that can be done in parallel\n",
    "            return AbstractPrivateTensor(None,\n",
    "                shares0 = x.shares0.dot(k),\n",
    "                shares1 = x.shares1.dot(k)\n",
    "            )\n",
    "        \n",
    "        def reduce(x):\n",
    "            return AbstractPrivateTensor(None,\n",
    "                shares0 = x.shares0.reduce(),\n",
    "                shares1 = x.shares1.reduce()\n",
    "            )\n",
    "        \n",
    "        # TODO need to implement `mod` on tensors first\n",
    "#         def truncate(x):\n",
    "#             return AbstractPrivateTensor(None,\n",
    "#                 shares0 = raw_truncate(x.share0),\n",
    "#                 shares1 = M_wrapped - raw_truncate(M_wrapped - x.share1)\n",
    "#             )\n",
    "            \n",
    "\n",
    "    return AbstractPrivateTensor\n",
    "\n",
    "for tensor_type in [NaturalTensor, CrtTensor]:\n",
    "    \n",
    "    PublicTensor  = tensor_type\n",
    "    PrivateTensor = gen_private_tensor(tensor_type)\n",
    "\n",
    "    a = np.array([ 2**120 for _ in range(1024) ])\n",
    "    b = np.array([ 2**110 for _ in range(1024) ])\n",
    "    x = PrivateTensor(a)\n",
    "    y = PrivateTensor(b)\n",
    "    k = PublicTensor(b)\n",
    "\n",
    "    z = x + y; z = z.reduce(); assert (z.unwrap() == (a+b) % M).all(), z\n",
    "    z = x - y; z = z.reduce(); assert (z.unwrap() == (a-b) % M).all(), z\n",
    "    z = x * k; z = z.reduce(); assert (z.unwrap() == (a*b) % M).all(), z\n",
    "    z = x.dot(k); z = z.reduce(); assert z.unwrap() == a.dot(b) % M, z\n",
    "    # z = y.truncate(); assert z.unwrap() in [b // K, b // K + 1], z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance tests\n",
    "\n",
    "Also ~10x speedup on single core device here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaturalTensor  : 0:00:02.330989\n",
      "CrtTensor      : 0:00:00.183384\n"
     ]
    }
   ],
   "source": [
    "for tensor_type in [NaturalTensor, CrtTensor]:\n",
    "    \n",
    "    PublicTensor  = tensor_type\n",
    "    PrivateTensor = gen_private_tensor(tensor_type)\n",
    "    \n",
    "    a = np.array([ 2**120 for _ in range(1024) ])\n",
    "    b = np.array([ 2**110 for _ in range(1024) ])\n",
    "    x = PrivateTensor(a)\n",
    "    k = PublicTensor(b)\n",
    "    \n",
    "    start = datetime.now()\n",
    "    for _ in range(10000):\n",
    "        z = x.dot(k)\n",
    "        #z.reduce() # only little effect compared to dot\n",
    "    end = datetime.now()\n",
    "    print('{:15}: {}'.format(tensor_type.__name__, end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
